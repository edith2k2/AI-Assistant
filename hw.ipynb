{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: langchain-google-community[gmail]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: pydantic[email]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (4.45.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (3.2.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -q langchain-google-community[gmail] duckduckgo-search langgraph langchain-community langchain-anthropic langchain anthropic\n",
    "%pip install -q langchain-google-calendar-tools\n",
    "%pip install -q pydantic[email]\n",
    "%pip install  transformers sentence-transformers faiss-cpu\n",
    "%pip install -q composio-langchain\n",
    "%pip install python-dotenv\n",
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%composio` not found.\n"
     ]
    }
   ],
   "source": [
    "%composio add googlecalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Manually reset the environment variable\n",
    "if \"ANTHROPIC_API_KEY\" in os.environ:\n",
    "    del os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()  # This loads the .env file in the current directory\n",
    "\n",
    "# Verify that the environment variable is loaded\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"API key not found. Make sure the .env file is properly configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Public model\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "claude_model = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started successfully.\n",
      "Ollama server is running.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the Ollama server\n",
    "import subprocess\n",
    "\n",
    "def start_ollama_server():\n",
    "    try:\n",
    "        # Run the \"ollama serve\" command\n",
    "        process = subprocess.Popen(\n",
    "            [\"ollama\", \"serve\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        print(\"Ollama server started successfully.\")\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Ollama server: {e}\")\n",
    "\n",
    "# Start the server\n",
    "ollama_process = start_ollama_server()\n",
    "\n",
    "def is_ollama_running():\n",
    "    try:\n",
    "        # Run the \"pgrep\" command to check for the \"ollama\" process\n",
    "        result = subprocess.run([\"pgrep\", \"-f\", \"ollama\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            print(\"Ollama server is running.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Ollama server is not running.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking if Ollama is running: {e}\")\n",
    "        return False\n",
    "is_ollama_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Set up Local LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llama_model = ChatOllama(model=\"llama3.2\")\n",
    "response = llama_model.invoke([HumanMessage(content=\"Are you there?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Gmail tools\n",
    "from langchain_community.tools.gmail.utils import (\n",
    "    build_resource_service,\n",
    "    get_gmail_credentials,\n",
    ")\n",
    "from langchain_google_community import GmailToolkit\n",
    "credentials = get_gmail_credentials(\n",
    "    token_file=\"token.json\",\n",
    "    scopes=[\"https://mail.google.com/\"],\n",
    "    client_secrets_file=\"credentials.json\",\n",
    ")\n",
    "api_resource = build_resource_service(credentials=credentials)\n",
    "toolkit = GmailToolkit(api_resource=api_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2024-10-25 16:18:34,359][INFO] Logging is set to INFO, use `logging_level` argument or `COMPOSIO_LOGGING_LEVEL` change this\n"
     ]
    }
   ],
   "source": [
    "# Set up google calendar tools\n",
    "from composio_langchain import ComposioToolSet, Action\n",
    "tool_set = ComposioToolSet()\n",
    "google_calendar_tools = tool_set.get_tools(actions=[\n",
    "    Action.GOOGLECALENDAR_CREATE_EVENT,\n",
    "    Action.GOOGLECALENDAR_DELETE_EVENT,\n",
    "    Action.GOOGLECALENDAR_FIND_FREE_SLOTS,\n",
    "    Action.GOOGLECALENDAR_UPDATE_EVENT\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Internet search tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "internet_search_tool = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/tvx0scm502l9b2_3n069p5080000gn/T/ipykernel_13507/4104372808.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Set up PDF tools\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load PDFs and create the vector store\n",
    "pdf_loader = PyPDFLoader(\"../Lectures/Lecture 1.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "vector_store = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# configure retreival QA\n",
    "pdf_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llama_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever()\n",
    ")\n",
    "# Add the PDF QA tool to the agent\n",
    "class PDFQATool:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "\n",
    "    def run(self, question):\n",
    "        return self.qa_chain.run(question)\n",
    "\n",
    "\n",
    "pdf_qa_tool = PDFQATool(pdf_qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tools\n",
    "tools = toolkit.get_tools()\n",
    "tools.append(internet_search_tool)\n",
    "tools.extend(google_calendar_tools)\n",
    "# tools.append(pdf_qa_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent with claude model\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "claude_memory = MemorySaver() # Initialize memory saver\n",
    "\n",
    "claude_agent = create_react_agent(claude_model, tools, checkpointer=claude_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "class Output(BaseModel):\n",
    "    tools: str = Field(description=\"Tools used\")\n",
    "    AI_message: str = Field(description=\"AI message\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_react_agent() missing 1 required positional argument: 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_react_agent\n\u001b[1;32m      2\u001b[0m chain \u001b[38;5;241m=\u001b[39m claude_model\n\u001b[0;32m----> 3\u001b[0m chained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_react_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_react_agent() missing 1 required positional argument: 'prompt'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "chain = claude_model\n",
    "chained_agent = create_react_agent(chain, tools, output_parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.pregel.io.AddableUpdatesDict'>\n",
      "{'agent': {'messages': [AIMessage(content='I\\'ve aimed to politely explain multiple times now that simply repeating \"hello\" without any actual question or request does not allow me to have a productive conversation or provide any meaningful assistance as an AI. Unless you are able to state a specific need you would like my help with, I don\\'t have any other way to constructively continue this interaction. Please feel free to start a new conversation if an actual query or task arises, but repeating open-ended greetings does not enable me to effectively serve my purpose. For now, I will have to end this exchange. Thank you for your understanding.', additional_kwargs={}, response_metadata={'id': 'msg_01VBmx8aU3JnUgdfVHaGWaYu', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 5812, 'output_tokens': 127}}, id='run-0737bd69-a14b-4a63-999a-600eba4e536a-0', usage_metadata={'input_tokens': 5812, 'output_tokens': 127, 'total_tokens': 5939, 'input_token_details': {}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config = {\"configurable\": {\"thread_id\": \"xyz456\"}}\n",
    "temp = \"hello \"\n",
    "for chunk in claude_agent.stream(\n",
    "{\"messages\": [HumanMessage(content=temp)]}, config\n",
    "):\n",
    "    data = chunk\n",
    "    print(type(data))\n",
    "    print(data)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent with llama model\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "llama_memory = MemorySaver() # Initialize memory saver\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "llama_agent = create_react_agent(llama_model, tools, checkpointer=llama_memory)#, state_modifier=SystemMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to analyze this message and determine if it contains private information, let's break it down step by step:\n",
      "\n",
      "1. break down the message and identify any specific pieces of information that might fall into one of these categories.\n",
      "\n",
      "the only piece of information in this message is \"my social security number is 123-45-6789.\"\n",
      "\n",
      "2. for each piece of information you identified, explain why it is or isn't considered private.\n",
      "\n",
      "a social security number is a personal identifier and falls under the category of private information. it's a sensitive piece of data that can be used to identify an individual and is typically protected by law for privacy purposes.\n",
      "\n",
      "3. based on your analysis, conclude whether the message contains private information.\n",
      "\n",
      "yes, this message contains private information because it includes a personal identifier (social security number).\n"
     ]
    }
   ],
   "source": [
    "def check_private_data_with_cot(message: str, local_llm: Callable) -> bool:\n",
    "    \"\"\"Check if the message contains private data using the local LLM with Chain of Thought prompting.\"\"\"\n",
    "    private_data_prompt = (\n",
    "        \"Analyze the following message to determine if it contains private or sensitive information. \"\n",
    "        \"Private information includes personal identifiers (e.g., social security numbers, passport numbers), \"\n",
    "        \"contact information (e.g., phone numbers, email addresses), financial details (e.g., credit card numbers, bank account information), \"\n",
    "        \"or personal health information. Here’s how you should think step by step:\\n\\n\"\n",
    "        \"1. Break down the message and identify any specific pieces of information that might fall into one of these categories.\\n\"\n",
    "        \"2. For each piece of information you identify, explain why it is or isn’t considered private.\\n\"\n",
    "        \"3. Based on your analysis, conclude whether the message contains private information. Respond with 'Yes' if private data is present, \"\n",
    "        \"or 'No' if it is not.\\n\\n\"\n",
    "        f\"Message: '{message}'\\n\"\n",
    "        \"Let's think step by step.\"\n",
    "    )\n",
    "    response = llama_model.invoke([HumanMessage(content=private_data_prompt)]).content\n",
    "    response = response.strip().lower()\n",
    "    print(response)\n",
    "    return \"yes\" in response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: 1. the main intent or purpose of the message is for the user to inquire about the current president of the united states.\n",
      "\n",
      "2. to provide a complete response, i need some additional information, such as \"what kind of date\" are they asking for (e.g., today, in 2023, etc.)? the ambiguity of this request could lead to an outdated or incorrect answer.\n",
      "\n",
      "3. no, the message is not clear and sufficient on its own because it lacks specific context regarding when the user wants to know about the current president.\n",
      "\n",
      "4. clarifying question: \"what date would you like to know the current president's status for?\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_message_clarity(message: str, llm: Callable) -> bool:\n",
    "    \"\"\"Check if the message is clear using the LLM.\"\"\"\n",
    "    clarity_prompt = (\n",
    "        f\"Analyze the following message to determine if it is clear and provides enough information to respond accurately. \"\n",
    "        f\"Follow these steps to reach your decision:\\n\"\n",
    "        f\"1. Identify the main intent or purpose of the message. State what the user is asking or communicating.\\n\"\n",
    "        f\"2. Check if all necessary information is present to provide a complete response. \"\n",
    "        f\"If there is missing or ambiguous information, note what is unclear.\\n\"\n",
    "        f\"3. Conclude with 'Yes' if the message is clear and sufficient for you to respond appropriately, or 'No' if it is ambiguous and requires clarification.\\n\"\n",
    "        f\"4. If you conclude with 'No', provide a specific clarifying question to obtain the missing information.\\n\\n\"\n",
    "        f\"Message: '{message}'\\n\"\n",
    "        f\"Let's think step by step.\"\n",
    "    )\n",
    "    # Get the LLM's response\n",
    "    response = llm.invoke([HumanMessage(content=clarity_prompt)]).content.strip().lower()\n",
    "    # Check the LLM's response for 'Yes' or 'No'\n",
    "    print( f\"LLM response: {response}\")\n",
    "    if \"need more information\" in response or \"clarify\" in response or \"unclear\" in response:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "message = \"Who is the president of the United States today?\"\n",
    "is_clear = check_message_clarity(message, llama_model)\n",
    "is_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# def check_message_clarity(message: str, llm: Callable) -> bool:\n",
    "#     \"\"\"Check if the message is clear using the LLM.\"\"\"\n",
    "#     clarity_prompt = (\n",
    "#         f\"The following message may contain an incomplete or unclear request. \"\n",
    "#         f\"Your task is to understand the intent of the message as much as possible. \"\n",
    "#         f\"If the message is clear, provide an appropriate response. \"\n",
    "#         f\"However, if the message is ambiguous or you need more information to respond accurately, \"\n",
    "#         f\"ask specific, clarifying questions to gather the necessary details. \"\n",
    "#         f\"Ensure your questions are direct and focused on obtaining the missing information needed to proceed effectively.\"\n",
    "#         f\"\\n\\nMessage: '{message}'\"\n",
    "#     )\n",
    "    \n",
    "#     # Get the LLM's response\n",
    "#     response = llama_model.invoke([HumanMessage(content=clarity_prompt)]).content\n",
    "#     # Check the LLM's response for 'Yes' or 'No'\n",
    "#     print( f\"LLM response: {response}\")\n",
    "#     response_cleaned = response.strip().lower()\n",
    "#     if \"need more information\" in response or \"clarify\" in response or \"unclear\" in response:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "def check_private_data_with_llm(message: str, llama_model: Callable) -> bool:\n",
    "    \"\"\"Check if the message contains private data using the local LLM.\"\"\"\n",
    "    # Create a prompt for the LLM\n",
    "    prompt = (\n",
    "            f\"Analyze the following message to determine if it contains private or sensitive information, such as personal identifiers \"\n",
    "            f\"(e.g., social security numbers, passport numbers), contact information (e.g., phone numbers, email addresses), \"\n",
    "            f\"financial details (e.g., credit card numbers, bank account information), or personal health information.\\n\\n\"\n",
    "            f\"Message: '{message}'\\n\\n\"\n",
    "            f\"Please respond with 'True' if the message contains private or sensitive information, or 'False' if it does not.\"\n",
    "        )\n",
    "    \n",
    "    # Get the LLM's response\n",
    "    response = llama_model.invoke([HumanMessage(content=prompt)]).content\n",
    "    # Check the LLM's response for 'Yes' or 'No'\n",
    "    print( f\"LLM response: {response}\")\n",
    "    response_cleaned = response.strip().lower()\n",
    "    if \"true\" in response_cleaned or \"yes\" in response_cleaned :\n",
    "        return True\n",
    "    elif \"no\" in response_cleaned:\n",
    "        return False\n",
    "    else :\n",
    "        # If the response is unclear, assume it may contain private data\n",
    "        return True\n",
    "    \n",
    "def process_message(message: str, local_llm: Callable) -> str:\n",
    "    \"\"\"Process the message based on whether it contains private data or needs clarification.\"\"\"\n",
    "    if check_private_data_with_llm(message, local_llm):\n",
    "        print(\"Private data detected. Using local agent.\")\n",
    "        return \"Local\"\n",
    "    else:\n",
    "        print(\"Message is clear and does not contain private data. Using public agent.\")\n",
    "        return \"Public\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_messages_from_file(filename, llama_model, llama_agent, claude_agent):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            messages = file.readlines()\n",
    "            for user_message in messages:\n",
    "                user_message = user_message.strip()\n",
    "                if not user_message:\n",
    "                    continue\n",
    "                \n",
    "                response = process_message(user_message, llama_model)\n",
    "                if response == \"Local\":\n",
    "                    config = {\"configurable\": {\"thread_id\": \"xyz123\"}}\n",
    "                    for chunk in llama_agent.stream(\n",
    "                        {\"messages\": [HumanMessage(content=user_message)]}, config\n",
    "                    ):\n",
    "                        print(chunk)\n",
    "                        print(\"----\")\n",
    "                else:\n",
    "                    config = {\"configurable\": {\"thread_id\": \"xyz456\"}}\n",
    "                    for chunk in claude_agent.stream(\n",
    "                        {\"messages\": [HumanMessage(content=user_message)]}, config\n",
    "                    ):\n",
    "                        print(chunk)\n",
    "                        print(\"----\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found. Please check the filename and try again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"Hello Bob, it's nice to meet you! I'm an AI assistant created by Anthropic. How are you doing today?\", additional_kwargs={}, response_metadata={'id': 'msg_01Un3aos9QeMb6BMCgPmpXy6', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 7364, 'output_tokens': 30}}, id='run-ae16854d-3d62-4a26-ac8a-2dcd87162a65-0', usage_metadata={'input_tokens': 7364, 'output_tokens': 30, 'total_tokens': 7394, 'input_token_details': {}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# config = {\"configurable\": {\"thread_id\": \"xyz456\"}}\n",
    "# temp = \"what are my events for tommorrow\"\n",
    "# def parse_chunk(chunk):\n",
    "#     if 'agent' in chunk:\n",
    "#         agent_messages = chunk['agent']['messages']\n",
    "#         for message in agent_messages:\n",
    "#             print(\"Agent Message:\")\n",
    "#             print(message.content)\n",
    "\n",
    "#     if 'tools' in chunk:\n",
    "#         tool_messages = chunk['tools']['messages']\n",
    "#         for message in tool_messages:\n",
    "#             print(\"Tool Message:\")\n",
    "#             print(f\"Tool: {message.name}\")\n",
    "#             print(f\"Content: {message.content}\")\n",
    "#             print(\"----\")\n",
    "\n",
    "# # Use this function in your loop\n",
    "# # response = claude_agent.invoke(\n",
    "# #     {\"messages\": [HumanMessage(content=\"whats the weather in austin?\")]}, config\n",
    "# # )\n",
    "\n",
    "# for chunk in claude_agent.stream(ls\n",
    "\n",
    "#     {\"messages\": [HumanMessage(content=\"hi im bob!\")]}, config\n",
    "# ):\n",
    "#     print(chunk)\n",
    "#     print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: True. The message contains a Social Security Number (SSN), which is considered private and sensitive information.\n",
      "Private data detected. Using local agent.\n",
      "{'agent': {'messages': [AIMessage(content=\"I can't help you with that. If you need assistance with updating your Social Security number, I suggest contacting your local Social Security administration office or visiting their website for more information. Is there anything else I can help you with?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-25T22:16:15.415444Z', 'message': {'role': 'assistant', 'content': \"I can't help you with that. If you need assistance with updating your Social Security number, I suggest contacting your local Social Security administration office or visiting their website for more information. Is there anything else I can help you with?\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 6861055958, 'load_duration': 45235792, 'prompt_eval_count': 1026, 'prompt_eval_duration': 4468922000, 'eval_count': 47, 'eval_duration': 2331379000}, id='run-f62b37f6-2e7d-4fee-b066-4d6f22fdfb51-0', usage_metadata={'input_tokens': 1026, 'output_tokens': 47, 'total_tokens': 1073})]}}\n",
      "----\n",
      "LLM response: False. The message does not contain any private or sensitive information. It appears to be a request for assistance with sending an email, and does not include any personal identifiers, contact information, financial details, or personal health information that would be considered private or sensitive.\n",
      "Message is clear and does not contain private data. Using public agent.\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"Sure Bob, I can help you send an email. Just provide me with the details like recipient(s), subject, and message content, and I'll use the Gmail tools to draft or send the email for you.\", 'type': 'text'}, {'id': 'toolu_01Vy1QFTbL4cJbRXLS2QZ7rY', 'input': {'to': 'Enter recipient email(s) here', 'subject': 'Enter subject line here', 'message': 'Enter your message content here'}, 'name': 'send_gmail_message', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01MEA3gcsuJuXPYhJonWogML', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 7405, 'output_tokens': 147}}, id='run-ec03ea07-f931-4fb6-8f5d-6b95f30f406b-0', tool_calls=[{'name': 'send_gmail_message', 'args': {'to': 'Enter recipient email(s) here', 'subject': 'Enter subject line here', 'message': 'Enter your message content here'}, 'id': 'toolu_01Vy1QFTbL4cJbRXLS2QZ7rY', 'type': 'tool_call'}], usage_metadata={'input_tokens': 7405, 'output_tokens': 147, 'total_tokens': 7552, 'input_token_details': {}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Error: Exception(\\'An error occurred: <HttpError 400 when requesting https://gmail.googleapis.com/gmail/v1/users/me/messages/send?alt=json returned \"Invalid To header\". Details: \"[{\\\\\\'message\\\\\\': \\\\\\'Invalid To header\\\\\\', \\\\\\'domain\\\\\\': \\\\\\'global\\\\\\', \\\\\\'reason\\\\\\': \\\\\\'invalidArgument\\\\\\'}]\">\\')\\n Please fix your mistakes.', name='send_gmail_message', id='015fac2d-5a21-4f60-a1cf-17b5ce5fb18d', tool_call_id='toolu_01Vy1QFTbL4cJbRXLS2QZ7rY')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"Oops, looks like I need actual values for the recipient, subject, and message fields to properly send an email. Let me try again - please provide those details and I'll get the email sent out for you.\", additional_kwargs={}, response_metadata={'id': 'msg_01EdW8oPRbwUyQQFwpVarhvZ', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 7655, 'output_tokens': 49}}, id='run-309a73ac-2bab-4387-a423-54623986c993-0', usage_metadata={'input_tokens': 7655, 'output_tokens': 49, 'total_tokens': 7704, 'input_token_details': {}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    user_message = input(\"Enter your message (type 'quit' to exit): \")\n",
    "    if user_message.lower() == 'quit':\n",
    "        break\n",
    "    elif user_message.lower() == 'load':\n",
    "        filename = \"messages.txt\"\n",
    "        process_messages_from_file(filename, llama_model, llama_agent, claude_agent)\n",
    "    else :\n",
    "    \n",
    "        response = process_message(user_message, llama_model)\n",
    "        if response == \"Local\":\n",
    "            config = { \"configurable\": {\"thread_id\": \"xyz123\"} }\n",
    "            for chunk in llama_agent.stream(\n",
    "                {\"messages\": [HumanMessage(content=user_message)]}, config\n",
    "            ):\n",
    "                print(chunk)\n",
    "                print(\"----\")\n",
    "        else:\n",
    "            config = {\"configurable\": {\"thread_id\": \"xyz456\"}}\n",
    "            for chunk in claude_agent.stream(\n",
    "                {\"messages\": [HumanMessage(content=user_message)]}, config\n",
    "            ):\n",
    "                print(chunk)\n",
    "                print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
