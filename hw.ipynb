{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: langchain-google-community[gmail]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: pydantic[email]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (4.45.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (3.2.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -q langchain-google-community[gmail] duckduckgo-search langgraph langchain-community langchain-anthropic langchain anthropic\n",
    "%pip install -q langchain-google-calendar-tools\n",
    "%pip install -q pydantic[email]\n",
    "%pip install  transformers sentence-transformers faiss-cpu\n",
    "%pip install -q composio-langchain\n",
    "%pip install python-dotenv\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install pypdf2\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%composio` not found.\n"
     ]
    }
   ],
   "source": [
    "%composio add googlecalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Manually reset the environment variable\n",
    "if \"ANTHROPIC_API_KEY\" in os.environ:\n",
    "    del os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()  # This loads the .env file in the current directory\n",
    "\n",
    "# Verify that the environment variable is loaded\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"API key not found. Make sure the .env file is properly configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Public model\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "claude_model = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I'm here! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "response = claude_model.invoke([HumanMessage(content=\"Are you there?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started successfully.\n",
      "Ollama server is running.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "def stop_ollama_processes():\n",
    "    try:\n",
    "        # Find all running Ollama processes\n",
    "        result = subprocess.run([\"pgrep\", \"-f\", \"ollama\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            pids = result.stdout.decode().splitlines()\n",
    "            for pid in pids:\n",
    "                # Terminate each process\n",
    "                subprocess.run([\"kill\", \"-15\", pid])\n",
    "            print(\"All Ollama processes have been stopped.\")\n",
    "            # Wait a moment to ensure processes have time to terminate\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print(\"No running Ollama processes found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error stopping Ollama processes: {e}\")\n",
    "\n",
    "def start_ollama_server():\n",
    "    try:\n",
    "        # Start the Ollama server\n",
    "        process = subprocess.Popen(\n",
    "            [\"ollama\", \"serve\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        print(\"Ollama server started successfully.\")\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Ollama server: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_ollama_running():\n",
    "    try:\n",
    "        # Try to list models, if successful, Ollama is running\n",
    "        ollama.list()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Start the Ollama server\n",
    "ollama_process = start_ollama_server()\n",
    "\n",
    "# Wait for the server to start\n",
    "while not is_ollama_running():\n",
    "    time.sleep(1)\n",
    "    print(\"Waiting for Ollama server to start...\")\n",
    "\n",
    "print(\"Ollama server is running.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Set up Local LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llama_model = ChatOllama(model=\"llama3.2\")\n",
    "response = llama_model.invoke([HumanMessage(content=\"Are you there?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Gmail tools\n",
    "from langchain_community.tools.gmail.utils import (\n",
    "    build_resource_service,\n",
    "    get_gmail_credentials,\n",
    ")\n",
    "from langchain_google_community import GmailToolkit\n",
    "credentials = get_gmail_credentials(\n",
    "    token_file=\"token.json\",\n",
    "    scopes=[\"https://mail.google.com/\"],\n",
    "    client_secrets_file=\"credentials.json\",\n",
    ")\n",
    "api_resource = build_resource_service(credentials=credentials)\n",
    "toolkit = GmailToolkit(api_resource=api_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-27 21:40:54,817][INFO] Logging is set to INFO, use `logging_level` argument or `COMPOSIO_LOGGING_LEVEL` change this\n"
     ]
    }
   ],
   "source": [
    "# Set up google calendar tools\n",
    "from composio_langchain import ComposioToolSet, Action\n",
    "tool_set = ComposioToolSet()\n",
    "google_calendar_tools = tool_set.get_tools(actions=[\n",
    "    Action.GOOGLECALENDAR_CREATE_EVENT,\n",
    "    Action.GOOGLECALENDAR_DELETE_EVENT,\n",
    "    Action.GOOGLECALENDAR_FIND_FREE_SLOTS,\n",
    "    Action.GOOGLECALENDAR_UPDATE_EVENT,\n",
    "    Action.GOOGLECALENDAR_FIND_EVENT,\n",
    "    Action.GOOGLECALENDAR_QUICK_ADD,\n",
    "    Action.GOOGLECALENDAR_GET_CURRENT_DATE_TIME\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Internet search tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "internet_search_tool = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/machine_learning/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from typing import Dict, Any\n",
    "\n",
    "class PDFQASystem:\n",
    "    def __init__(self):\n",
    "        self.pdf_dir = \"pdf_documents\"\n",
    "        os.makedirs(self.pdf_dir, exist_ok=True)\n",
    "        \n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        self.vectorstore = Chroma(embedding_function=self.embeddings, persist_directory=\"./chroma_db\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        \n",
    "        self.qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm=claude_model,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        )\n",
    "        \n",
    "        # Add a set to keep track of added PDFs\n",
    "        self.added_pdfs = set()\n",
    "\n",
    "    def check_if_pdf_valid(self, file_path: str) -> bool:\n",
    "        return os.path.exists(file_path) and file_path.lower().endswith('.pdf')\n",
    "\n",
    "    def add_pdf(self, file_path):\n",
    "        # Check if the PDF has already been added\n",
    "        if file_path in self.added_pdfs:\n",
    "            print(f\"PDF '{file_path}' has already been added to the system.\")\n",
    "            return True\n",
    "\n",
    "        if not self.check_if_pdf_valid(file_path):\n",
    "            print(f\"Error: '{file_path}' is not a valid PDF file or does not exist.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            splits = self.text_splitter.split_documents(documents)\n",
    "            self.vectorstore.add_documents(splits)\n",
    "            \n",
    "            # Add the file path to the set of added PDFs\n",
    "            self.added_pdfs.add(file_path)\n",
    "            \n",
    "            print(f\"Added {file_path} to the system.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding PDF '{file_path}': {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def format_output(self, result: Dict[str, Any]) -> str:\n",
    "        formatted = \"=\" * 50 + \"\\n\"\n",
    "        formatted += f\"Question: {result['question']}\\n\"\n",
    "        formatted += \"=\" * 50 + \"\\n\\n\"\n",
    "        \n",
    "        formatted += \"Answer:\\n\"\n",
    "        formatted += \"-\" * 50 + \"\\n\"\n",
    "        formatted += f\"{result['answer']}\\n\\n\"\n",
    "        formatted += \"-\" * 50 + \"\\n\"\n",
    "        \n",
    "        formatted += \"Sources:\\n\"\n",
    "        formatted += \"-\" * 50 + \"\\n\"\n",
    "        for source in result['sources'].split(\", \"):\n",
    "            formatted += f\"- {source}\\n\"\n",
    "        formatted += \"-\" * 50 + \"\\n\"\n",
    "\n",
    "        return formatted\n",
    "    def query(self, question):\n",
    "        result = self.qa_chain({\"question\": question})\n",
    "        print(self.format_output(result))\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "qa_system = PDFQASystem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tools\n",
    "tools = toolkit.get_tools()\n",
    "tools.extend(google_calendar_tools)\n",
    "llama_tools = tools.copy()\n",
    "tools.append(internet_search_tool)\n",
    "claude_tools = tools.copy()\n",
    "# tools.append(pdf_qa_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent with claude model\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "claude_memory = MemorySaver() # Initialize memory saver\n",
    "\n",
    "claude_agent = create_react_agent(claude_model, claude_tools, checkpointer=claude_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(\"Tool Called\")\n",
    "        \n",
    "        elif isinstance(message, dict) and 'text' in message:\n",
    "            # Handle AI text response\n",
    "            print(\"AI Response:\")\n",
    "            print(message['text'])\n",
    "        \n",
    "        elif isinstance(message, dict) and message.get(\"type\") == \"tool_use\":\n",
    "            # Handle tool use\n",
    "            tool_name = message.get(\"name\", \"Unknown Tool\")\n",
    "            query = message.get(\"input\", {}).get(\"query\", \"No query provided\")\n",
    "            print(f\"Tool Called: {tool_name}\")\n",
    "            print(f\"Query: {query}\")\n",
    "        \n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what the previous question I asked?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Unfortunately, I don't have enough context to know what your previous question was since this is the start of our conversation. Could you please restate your question?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config = {\"configurable\": {\"thread_id\": \"xyz456\"}}\n",
    "temp = \"what the previous question I asked?\"\n",
    "message = {\"messages\": [('user', temp)]}\n",
    "print_stream(claude_agent.stream(message, stream_mode=\"values\", config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent with llama model\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "llama_memory = MemorySaver() # Initialize memory saver\n",
    "\n",
    "llama_agent = create_react_agent(llama_model, llama_tools, checkpointer=llama_memory)#, state_modifier=SystemMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_agent.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "write an email to 2k2vamshi@gmail.com saying HI\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is the JSON payload for sending an email:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"message\": {\n",
      "    \"text\": {\n",
      "      \"body\": \"HI\",\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  },\n",
      "  \"recipient\": {\n",
      "    \"address\": {\n",
      "      \"emailAddress\": \"2k2vamshi@gmail.com\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "And here is the request to send this email:\n",
      "\n",
      "```\n",
      "POST /users/{USER_ID}/messages HTTP/1.1\n",
      "Host: api.example.com\n",
      "Content-Type: application/json\n",
      "\n",
      "{\n",
      "  \"message\": {\n",
      "    \"text\": {\n",
      "      \"body\": \"HI\",\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  },\n",
      "  \"recipient\": {\n",
      "    \"address\": {\n",
      "      \"emailAddress\": \"2k2vamshi@gmail.com\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Please replace `{USER_ID}` with your actual user ID and `api.example.com` with the actual API endpoint.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config = {\"configurable\": {\"thread_id\": \"xyz456\"}}\n",
    "temp = \"write an email to 2k2vamshi@gmail.com saying HI\"\n",
    "message = {\"messages\": [('user', temp)]}\n",
    "print_stream(llama_agent.stream(message, stream_mode=\"values\", config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def check_if_vague(message: str, llama_model: Callable) -> bool:\n",
    "    \"\"\"Check if the message is unclear, vague, or needs more information using the local LLM.\"\"\"\n",
    "    print(f\"Checking if the message is vague... \")\n",
    "    prompt = (\n",
    "        \"Determine if the following message is vague or clear. A message is vague only if it's truly ambiguous \"\n",
    "        \"or lacks essential context that most people would need to understand it. \"\n",
    "        \"Simple questions about well-known topics, places, or common knowledge are generally clear.\\n\\n\"\n",
    "        f\"Message: '{message}'\\n\\n\"\n",
    "        \"Consider:\\n\"\n",
    "        \"1. Would an average person understand what's being asked or stated?\\n\"\n",
    "        \"2. Is it a straightforward question about a widely known topic or place?\\n\"\n",
    "        \"3. Does it require highly specific or personal information to be understood?\\n\\n\"\n",
    "        \"Respond with:\\n\"\n",
    "        \"CLEAR: If the message is understandable to most people or asks about common knowledge.\\n\"\n",
    "        \"VAGUE: Only if the message is truly ambiguous or lacks context that most people would need.\\n\"\n",
    "        \"Briefly explain your decision in one sentence.\"\n",
    "    )\n",
    "    \n",
    "    response = claude_model.invoke([HumanMessage(content=prompt)])\n",
    "    response.pretty_print()\n",
    "    \n",
    "    response_lower = response.content.lower()\n",
    "    if \"vague:\" in response_lower:\n",
    "        return True\n",
    "    elif \"clear:\" in response_lower:\n",
    "        return False\n",
    "    else:\n",
    "        # If the response doesn't clearly indicate, assume it's clear\n",
    "        print(\"Model response was unclear. Assuming the message is clear.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_if_pdf_related(message: str, llama_model: Callable) -> bool:\n",
    "    \"\"\"Check if the message is specifically related to PDF or document-based QA using the local LLM.\"\"\"\n",
    "    print(f\"Checking if the message is PDF-related...\")\n",
    "    prompt = (\n",
    "        \"Determine if the following message is specifically about PDFs or document-based questions. \"\n",
    "        \"Only respond with TRUE if the message explicitly mentions or clearly implies dealing with PDFs, \"\n",
    "        \"documents, or document analysis. Otherwise, respond with FALSE.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"- 'Can you summarize this PDF?' - TRUE\\n\"\n",
    "        \"- 'What does page 5 of the document say?' - TRUE\\n\"\n",
    "        \"- 'Extract information from this report.' - TRUE\\n\"\n",
    "        \"- 'What's the weather like today?' - FALSE\\n\"\n",
    "        \"- 'Who won the last World Cup?' - FALSE\\n\\n\"\n",
    "        f\"Message: '{message}'\\n\\n\"\n",
    "        \"Respond with:\\n\"\n",
    "        \"TRUE: Only if the message explicitly relates to PDFs or document analysis.\\n\"\n",
    "        \"FALSE: If the message does not specifically mention or imply dealing with documents.\\n\"\n",
    "        \"Briefly explain your decision.\"\n",
    "    )\n",
    "    \n",
    "    response = claude_model.invoke([HumanMessage(content=prompt)])\n",
    "    response.pretty_print()\n",
    "    \n",
    "    response_lower = response.content.lower()\n",
    "    if \"true\" in response_lower:\n",
    "        return True\n",
    "    elif \"false\" in response_lower:\n",
    "        return False\n",
    "    else:\n",
    "        # If the response is ambiguous, assume it's not PDF-related\n",
    "        print(\"Model response was unclear. Assuming the message is not PDF-related.\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "def check_private_data_with_llm(message: str, llama_model: Callable) -> bool:\n",
    "    \"\"\"Check if the message contains private data using the local LLM.\"\"\"\n",
    "    # Create a prompt for the LLM\n",
    "    print(f\"Checking for private data in message...\")\n",
    "    prompt = (\n",
    "             \"You are a privacy expert tasked with identifying sensitive information in messages. \"\n",
    "        \"Analyze the following message step by step to determine if it contains any private or sensitive information. \"\n",
    "        \"Consider these categories:\\n\"\n",
    "        \"1. Personal identifiers (e.g., social security numbers, passport numbers)\\n\"\n",
    "        \"2. Contact information (e.g., phone numbers, email addresses)\\n\"\n",
    "        \"3. Financial details (e.g., credit card numbers, bank account information)\\n\"\n",
    "        \"4. Personal health information (e.g., medical conditions, prescription details)\\n\"\n",
    "        \"5. Confidential business information (e.g., trade secrets, unreleased product details)\\n\"\n",
    "        \"6. Login credentials (e.g., usernames, passwords)\\n\\n\"\n",
    "        f\"Message: '{message}'\\n\\n\"\n",
    "        \"Follow these steps in your analysis:\\n\"\n",
    "        \"1. Identify any potential sensitive information in the message.\\n\"\n",
    "        \"2. Categorize each piece of information you've identified.\\n\"\n",
    "        \"3. Explain why each piece is or isn't considered sensitive.\\n\"\n",
    "        \"4. Make a final decision based on your analysis.\\n\\n\"\n",
    "        \"After your analysis, conclude with one of these statements:\\n\"\n",
    "        \"- FINAL_DECISION: True (if any sensitive information is found)\\n\"\n",
    "        \"- FINAL_DECISION: False (if no sensitive information is found)\\n\"\n",
    "        \"- FINAL_DECISION: Uncertain (if you're unsure about any part of the message)\"\n",
    "        )\n",
    "    \n",
    "    # Get the LLM's response\n",
    "    response = llama_model.invoke([HumanMessage(content=prompt)])\n",
    "    response.pretty_print()\n",
    "    # Check the LLM's response for 'Yes' or 'No'\n",
    "    response_cleaned = response.content.strip().lower()\n",
    "    if \"true\" in response_cleaned or \"yes\" in response_cleaned :\n",
    "        return True\n",
    "    elif \"no\" in response_cleaned:\n",
    "        return False\n",
    "    else :\n",
    "        # If the response is unclear, assume it may contain private data\n",
    "        return True\n",
    "    \n",
    "def process_message(message: str, local_llm: Callable) -> str:\n",
    "    \"\"\"Process the message to determine which agent to use based on private data, PDF-related questions, or vague queries.\"\"\"\n",
    "    # Check if the message contains private data\n",
    "    if check_private_data_with_llm(message, local_llm):\n",
    "        print(\"Private data detected. Using local agent.\")\n",
    "        return \"Local\"\n",
    "    \n",
    "    # Check if the message is vague or unclear\n",
    "    elif check_if_vague(message, local_llm):\n",
    "        print(\"Query is unclear or vague. Using local agent for clarification.\")\n",
    "        return \"Vague\"\n",
    "    \n",
    "    # Check if the message is related to PDF or document-based QA\n",
    "    elif check_if_pdf_related(message, local_llm):\n",
    "        print(\"PDF-related query detected. Using PDF agent.\")\n",
    "        return \"PDF\"\n",
    "    \n",
    "    # If none of the above conditions are met, use the public agent\n",
    "    else:\n",
    "        print(\"Message is clear, does not contain private data, and is not PDF-related. Using public agent.\")\n",
    "        return \"Public\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import Callable\n",
    "from langchain_core.messages import HumanMessage\n",
    "import re\n",
    "\n",
    "def process_message(message: str, local_llm: Callable) -> str:\n",
    "    \"\"\"Process the message to determine which agent to use based on private data, vague queries, or PDF-related questions.\"\"\"\n",
    "    if \"<private>\" in message.lower() or \"<sensitive>\" in message.lower() or \"<confidential>\" in message.lower():\n",
    "        print(\"Private data detected. Using local agent.\")\n",
    "        return \"Local\"\n",
    "    if \"<pdf>\" in message.lower() or \"<document>\" in message.lower(): \n",
    "        print(\"PDF-related query detected. Using PDF agent.\")\n",
    "        return \"PDF\"\n",
    "    if \"<public>\" in message.lower() or \"<email>\" in message.lower() or \"<contact>\" in message.lower():\n",
    "        print(\"Public query detected. Using public agent.\")\n",
    "        return \"Public\"\n",
    "    \n",
    "    prompt = (\n",
    "        \"Please analyze the following message step by step to determine its classification based on specific criteria. \"\n",
    "        \"Go through each step and follow the instructions carefully. Your response should end with exactly one tag: `<PRIVATE>`, `<VAGUE>`, `<PDF>`, or `<PUBLIC>`.\\n\\n\"\n",
    "        \n",
    "        \"Message Analysis:\\n\"\n",
    "        f\"Message: '{message}'\\n\\n\"\n",
    "\n",
    "        \"Step-by-Step Evaluation:\\n\\n\"\n",
    "        \n",
    "        \"Step 1: Privacy Check\\n\"\n",
    "        \"- Question: Does the message contain any private or sensitive data, such as personal identifiers, financial information, or health details?\\n\"\n",
    "        \"- Answer: If yes, the message should be classified as `<PRIVATE>`. If not, move to the next step.\\n\\n\"\n",
    "        \n",
    "        \"Step 2: Clarity Assessment\\n\"\n",
    "        \"- Question: Is the message vague or unclear, lacking sufficient detail or context to be fully understood?\\n\"\n",
    "        \"- Answer: If yes, the message should be classified as `<VAGUE>`. If not, continue to the next step.\\n\\n\"\n",
    "        \n",
    "        \"Step 3: PDF Relevance\\n\"\n",
    "        \"- Question: Does the message specifically pertain to PDFs, such as a request to retrieve, search, or answer questions based on PDF content?\\n\"\n",
    "        \"- Answer: If yes, classify the message as `<PDF>`. If not, proceed to the next step.\\n\\n\"\n",
    "        \n",
    "        \"Step 4: Final Classification\\n\"\n",
    "        \"- Question: If none of the previous tags apply, does the message lack private data, is it clear, and is not PDF-related?\\n\"\n",
    "        \"- Answer: If yes, classify the message as `<PUBLIC>`.\\n\\n\"\n",
    "        \n",
    "        \"Final Instruction:\\n\"\n",
    "        \"After considering each step carefully, respond with only the classification tag: `<PRIVATE>`, `<VAGUE>`, `<PDF>`, or `<PUBLIC>`.\\n\"\n",
    "    )\n",
    "    print(\"\\n================================ \", end = \"\")\n",
    "    print(\"Classifying message...\", end = \" \")\n",
    "    print(\" ================================\")\n",
    "    # Get the LLM's response\n",
    "    response = local_llm.invoke([HumanMessage(content=prompt)])\n",
    "    # .content\n",
    "    response.pretty_print()\n",
    "    response = response.content.strip()\n",
    "    \n",
    "    # Extract only the final standalone tag\n",
    "    matches = re.findall(r\"<(PRIVATE|VAGUE|PDF|PUBLIC)>\", response, re.IGNORECASE)\n",
    "    if matches:\n",
    "        classification = f\"<{matches[-1].upper()}>\"\n",
    "        print(f\"Detected classification: {classification}\")\n",
    "        \n",
    "        if classification == \"<PRIVATE>\":\n",
    "            return \"Local\"\n",
    "        elif classification == \"<VAGUE>\":\n",
    "            return \"Vague\"\n",
    "        elif classification == \"<PDF>\":\n",
    "            return \"PDF\"\n",
    "        else:\n",
    "            return \"Public\"\n",
    "    else:\n",
    "        print(\"No valid tag detected. Defaulting to Public.\")\n",
    "        return \"Public\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================ Classifying message...  ================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Step 1: Privacy Check\n",
      "- The message '/quit' does not contain any personal identifiers, financial information, or health details. Therefore, it does not meet the criteria for being classified as '<PRIVATE>'.\n",
      "\n",
      "Step 2: Clarity Assessment\n",
      "- The message '/quit' is clear and concise. It contains a specific command or action that can be easily understood by those familiar with the context. Therefore, it does not meet the criteria for being classified as '<VAGUE>'.\n",
      "\n",
      "Step 3: PDF Relevance\n",
      "- The message '/quit' does not specifically pertain to PDFs. It is a general command or action that may be used in various contexts, but it is not related to PDF content. Therefore, it does not meet the criteria for being classified as '<PDF>'.\n",
      "\n",
      "Step 4: Final Classification\n",
      "- Since the message lacks private data, is clear, and is not PDF-related, it meets the criteria for being classified as '<PUBLIC>'.\n",
      "Detected classification: <PUBLIC>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "/quit\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, goodbye!\n",
      "\n",
      "================================ Classifying message...  ================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Following the instructions:\n",
      "\n",
      "Step 1: Privacy Check\n",
      "- The message '/bye' does not contain any private or sensitive data.\n",
      "\n",
      "Step 2: Clarity Assessment\n",
      "- The message '/bye' is clear and easily understood, despite being a short and simple response.\n",
      "\n",
      "Step 3: PDF Relevance\n",
      "- The message '/bye' does not pertain to PDFs in any way.\n",
      "\n",
      "Step 4: Final Classification\n",
      "- Since the message lacks private data, is clear, and is not PDF-related, it can be classified as <PUBLIC>.\n",
      "\n",
      "<PUBLIC>\n",
      "Detected classification: <PUBLIC>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " /bye\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.formatted_text import HTML\n",
    "\n",
    "# Agent Configuration\n",
    "def get_agent_config(thread_id: str):\n",
    "    return {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "def extract_pdf_paths(user_message):\n",
    "    # Regular expression to identify PDF paths in the user's message\n",
    "    pdf_paths = re.findall(r'\\b(\\S+\\.pdf)\\b', user_message)\n",
    "    return pdf_paths\n",
    "\n",
    "def get_user_input():\n",
    "    placeholder_text = HTML('<style fg=\"gray\">Send a message (or /bye for exit)</style>')\n",
    "    # Display the placeholder, which disappears on typing\n",
    "    user_input = prompt('>> ', placeholder=placeholder_text)\n",
    "    return user_input\n",
    "\n",
    "# Main interaction loop\n",
    "while True:\n",
    "    user_message = input(\"Enter your message (or 'quit' to exit): \")\n",
    "    if user_message.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    response_type = process_message(user_message, llama_model)\n",
    "    message = {\"messages\": [('user', user_message)]}\n",
    "    \n",
    "    if response_type == \"Local\":\n",
    "        config = get_agent_config(\"xyz123\")\n",
    "        try:\n",
    "            print_stream(llama_agent.stream(message, stream_mode=\"values\", config=config))\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Local agent: {e}\")\n",
    "    \n",
    "    elif response_type == \"Vague\":\n",
    "        print(\"Your query was unclear. Please provide more detail.\")\n",
    "    \n",
    "    elif response_type == \"PDF\":\n",
    "        pdf_files = extract_pdf_paths(user_message)\n",
    "        try:\n",
    "            # Use the PDF QA system\n",
    "            status = True\n",
    "            for file in pdf_files:\n",
    "                status = qa_system.add_pdf(file)\n",
    "                if status == False:\n",
    "                    break\n",
    "            if status == False:\n",
    "                continue\n",
    "            result = qa_system.query(user_message)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with PDF QA system: {e}\")\n",
    "    \n",
    "    else:\n",
    "        config = get_agent_config(\"xyz456\")\n",
    "        try:\n",
    "            print_stream(claude_agent.stream(message, stream_mode=\"values\", config=config))\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Public agent: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
